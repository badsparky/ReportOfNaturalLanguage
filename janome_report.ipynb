{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Sentence Vecの作り方（2022.12月少し修正版）\n",
        "import math\n",
        "import statistics"
      ],
      "metadata": {
        "id": "cFcbRjKssyBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9KRNgv6seoB",
        "outputId": "aa65e6c5-a14c-418f-9b85-bb8d2024b7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install janome"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ここで、分析対象テキストファイルをアップロードします。"
      ],
      "metadata": {
        "id": "aY126wef0y5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#形態素解析をしたファイルを作成\n",
        "from janome.tokenizer import Tokenizer\n",
        "def split_text(src, out):\n",
        "    t = Tokenizer()\n",
        "    with open(src, encoding='utf8') as fin:\n",
        "        with open(out, mode='w', encoding='utf8') as fout:\n",
        "            for line in fin:\n",
        "              tokens = t.tokenize(line)\n",
        "              for token in tokens:\n",
        "                fout.write('%s\\n' % token)\n"
      ],
      "metadata": {
        "id": "rIJS77kxskrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 自分のファイルに変更する時は以下のutf8gingatetsudono_yoru.txtを変更する。\n",
        "split_text('/content/text.txt','tsv.out')\n"
      ],
      "metadata": {
        "id": "hp7x1KpawNlJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "e44e6da2-921f-46bd-f6de-eb2428cc8fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-86484572435d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 自分のファイルに変更する時は以下のutf8gingatetsudono_yoru.txtを変更する。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/text.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tsv.out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3f95fdce1a1c>\u001b[0m in \u001b[0;36msplit_text\u001b[0;34m(src, out)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/text.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GX_P8Oub_6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -50 tsv.out"
      ],
      "metadata": {
        "id": "XhQJSSOctaVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#形態素解析した結果ファイルを使い、形態素スペース区切りの本文テキストを出力する\n",
        "def sepfile_from(src,out):\n",
        "  with open(src, encoding='utf8') as fin:\n",
        "    with open(out, mode = 'w', encoding='utf8') as fout:\n",
        "      words = []\n",
        "      for line in fin:\n",
        "        a = line.split('\\t')\n",
        "        # 語の出現形だけでなく、品詞も考慮\n",
        "        pos = a[1].split(',')\n",
        "        words.append(a[0]+\"-\"+pos[0])\n",
        "        if a[0] == u'。':\n",
        "          #ファイルの最後にも「。」がないと処理しない\n",
        "          fout.write('%s\\n' % ' '.join(words))\n",
        "          words = []\n",
        "\n",
        "sepfile_from('/content/tsv.out','sent.txt')\n"
      ],
      "metadata": {
        "id": "oHUFUaYUttpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -50 sent.txt"
      ],
      "metadata": {
        "id": "3qz23a6Lt4H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#形態素スペース区切り本文ファイルの語彙リストを作る\n",
        "from collections import Counter\n",
        "def count_vocabdist(src,out):\n",
        "  with open(src, encoding='utf8') as fin:\n",
        "    text = fin.read()\n",
        "    #split関数は引数なしの時は、改行、タブ、スペース等を区切り記号にみなしてくれる。\n",
        "    words = text.split()\n",
        "    wdist = Counter(words)\n",
        "  with open(out,mode='w', encoding='utf8') as fout:\n",
        "      for k in wdist:\n",
        "        hinshi= k.split('-')[-1]\n",
        "        word=k.split('-')[0]\n",
        "        fout.write('%s\\t%s\\n' % (k,wdist[k]))\n",
        "\n",
        "def count_vocabdist2(src,out):\n",
        "  with open(src, encoding='utf8') as fin:\n",
        "    text = fin.read()\n",
        "    #split関数は引数なしの時は、改行、タブ、スペース等を区切り記号にみなしてくれる。\n",
        "    words = text.split()\n",
        "    wdist = Counter(words)\n",
        "  with open(out,mode='w', encoding='utf8') as fout:\n",
        "      for k in wdist:\n",
        "        hinshi= k.split('-')[-1]\n",
        "        word=k.split('-')[0]\n",
        "        if (hinshi==\"名詞\" or hinshi==\"形容詞\") and wdist[k]>6 and (word!='(' and word!=')' and word!='こと'and word!='これ')and len(word)>1:\n",
        "          fout.write('%s\\t%s\\n' % (k,wdist[k]))\n",
        "\n",
        "count_vocabdist('/content/sent.txt','vocadist.tsv')\n",
        "count_vocabdist2('/content/sent.txt','vocadist2.tsv')\n"
      ],
      "metadata": {
        "id": "L62UJOF0t7x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -50 vocadist2.tsv"
      ],
      "metadata": {
        "id": "WbzFOeCzuMCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ここでvocadist.tsvをダウンロードし、\n",
        "# google spreadsheetなりで、対象語リストを作成してください\n",
        "# 作成した対象語リストはvocadist2.tsvとしてください。"
      ],
      "metadata": {
        "id": "ah0lfAozuRrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#スペース区切り本文テキストと語彙リストから文ベクトルを作る\n",
        "def make_sentvec(src, voca, out):\n",
        "  vocabhash={}\n",
        "  vocablist=[]\n",
        "  with open(voca, encoding='utf8') as fv:\n",
        "    for text in fv:\n",
        "      a = text.split()\n",
        "      vocablist.append(a[0])\n",
        "      vocabhash[a[0]]=len(vocabhash)\n",
        "  #\n",
        "  fo = open(out, mode='w', encoding='utf8')\n",
        "  fo.write('S#')\n",
        "  for w in vocablist:\n",
        "    fo.write('\\t%s' % w)\n",
        "  fo.write('\\n')\n",
        "  # \n",
        "  sno = 0\n",
        "  with open(src, encoding='utf8') as fin:\n",
        "    for line in fin:\n",
        "      vec = [0]*len(vocabhash)\n",
        "      a = line.split()\n",
        "      for w in a:\n",
        "        if w in vocabhash:\n",
        "          vec[vocabhash[w]] +=1\n",
        "      fo.write('%d' % sno)\n",
        "      for d in vec:\n",
        "        fo.write('\\t%d' % d)\n",
        "      fo.write('\\n')\n",
        "      sno+=1\n",
        "  fo.close()\n",
        "\n",
        "make_sentvec('sent.txt','vocadist2.tsv','sentvec.txt')\n"
      ],
      "metadata": {
        "id": "fD0_Vmy3yEFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpGa6V5XJ-kf"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwghhUHMKFG0"
      },
      "source": [
        "#tsvの文ベクトルファイルをデータフレームとして読み込む\n",
        "df = pd.read_table('sentvec.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPnfphM9NVDe"
      },
      "source": [
        "#1列目の行番号フィールド列を削除する。\n",
        "df2 = df.drop('S#',axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWvfi-nUdNBO"
      },
      "source": [
        "#データフレームの列名を配列にする:names。\n",
        "#語とidとの辞書を作る: wordiddic\n",
        "names = df2.columns.values\n",
        "wordiddic ={}\n",
        "for w in names:\n",
        "  wordiddic[w] = len(wordiddic)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-H72dIZMaHo"
      },
      "source": [
        "#numpyの2次元配列に変換する。\n",
        "#\n",
        "mata = df2.to_numpy()\n",
        "mata\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ_jDCLoMqmV"
      },
      "source": [
        "#転置行列に元の行列を掛ける\n",
        "#ある種の語と語と間の相関行列が計算される\n",
        "corrmat= np.matmul(mata.T,mata)\n",
        "corrmat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-1POZDFSTdm"
      },
      "source": [
        "#単語の共起がある:1 か、ない:0だけで示す共起行列を作る\n",
        "r, c = corrmat.shape\n",
        "corrmat1=np.zeros((r,c))\n",
        "#原始的なループですみません、、、\n",
        "for i in range(r):\n",
        "  for j in range(c):\n",
        "    corrmat1[i][j]=1 if corrmat[i][j] > 0 else 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights={}\n",
        "with open(\"vocadist2.tsv\", encoding='utf8') as f:\n",
        "  for txt in f:\n",
        "    name=txt.split()[0]\n",
        "    weight=txt.split()[-1]\n",
        "    weights[name]=int(weight)\n",
        "print(weights)\n"
      ],
      "metadata": {
        "id": "Jh8RoAb__Zno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jrno72doeXM"
      },
      "source": [
        "from IPython.core.display import Math\n",
        "#任意の語に対するすべての語のユークリッド距離の算出\n",
        "\n",
        "list_f=[]\n",
        "\n",
        "for n in names:\n",
        "  target = n\n",
        "  simlist = {}\n",
        "  for w in names:\n",
        "    simlist[w] = np.sqrt(np.power(corrmat1[wordiddic[target]]-corrmat1[wordiddic[w]],2).sum())  \n",
        "  #昇順にソートして表示\n",
        "  a= sorted(simlist.items(),key=lambda x:x[1])\n",
        "  for i in range(0,10):\n",
        "    list_f.append((n,a[-1-i],a[-1-i][-1]))\n",
        "\n",
        "tmp_f=sorted(list_f,key=lambda x:x[-1],reverse=True)\n",
        "result_f= []\n",
        "for i,f in zip(range(len(tmp_f)),tmp_f):\n",
        "  result_f.append((f[0],f[1][0],f[-1]))\n",
        "\n",
        "print(\"対照\",result_f,sep='\\n')\n",
        "print(\"ベクトルの次元\",len(names),sep=\"\\n\")\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idoc_EhUPyVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "c2e43ad1-02e3-4623-eb30-cc95f92530a1"
      },
      "source": [
        "from IPython.core.display import Math\n",
        "#任意の語に対するすべての語のユークリッド距離の算出\n",
        "\n",
        "list_f=[]\n",
        "\n",
        "for n in names:\n",
        "  target = n\n",
        "  simlist = {}\n",
        "  for w in names:\n",
        "    simlist[w] = np.dot(corrmat1[wordiddic[target]],corrmat1[wordiddic[w]])\n",
        "  tmp=simlist.items\n",
        "  #昇順にソートして表示\n",
        "  a= sorted(simlist.items(),key=lambda x:x[1])\n",
        "  for item in a:\n",
        "    if item[-1]<0:\n",
        "      list_f.append((n,item,item[-1]))\n",
        "\n",
        "tmp_f=sorted(list_f,key=lambda x:x[-1],reverse=False)\n",
        "result_f= []\n",
        "for i,f in zip(range(len(tmp_f)),tmp_f):\n",
        "  result_f.append((f[0],f[1][0],f[-1]))\n",
        "\n",
        "print(\"対照\",result_f,sep='\\n')\n",
        "print(\"ベクトルの次元\",len(names),sep=\"\\n\")\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-dc2b722dad44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0msimlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'names' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Part I(準備編）\n",
        "#\n",
        "\n",
        "#ベクトル辞書のダウンロード\n",
        "!wget https://sudachi.s3-ap-northeast-1.amazonaws.com/chive/chive-1.2-mc30_gensim.tar.gz\n",
        "#ベクトル辞書の定義\n",
        "import gensim\n",
        "vecdic = \"/content/chive-1.2-mc30_gensim/chive-1.2-mc30.kv\"\n",
        "\n",
        "vectors = gensim.models.KeyedVectors.load(vecdic)\n",
        "\n"
      ],
      "metadata": {
        "id": "iDEiowTCVOra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#対照性の実験\n",
        "print(vectors.most_similar(positive=[\"アンドロイド\",\"バックアップ\"], negative=[\"グーグル\"],topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueHqmFPGI-qB",
        "outputId": "c7620ea6-08e2-497b-bbd0-b569eaee46c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('リストア', 0.5200445055961609), ('フルバックアップ', 0.4968959093093872), ('backup', 0.4944484233856201), ('ックアップ', 0.4928125739097595), ('trueimage', 0.48917585611343384)]\n"
          ]
        }
      ]
    }
  ]
}